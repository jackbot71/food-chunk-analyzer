# Food Chunk Analyzer

This project aims to program a computer vision model capable of analyzing a picture of loosely packaged food chunks (here coated peanuts) spread on a white background in order to evaluate product quality metrics such as the proportion of defects and the dimension distributions.


## Table of contents

1. [Installation](#installation)
2. [Project Motivation](#motivation)
3. [File Descriptions](#files)
4. [Results](#results)
5. [Licensing, Authors, and Acknowledgements](#licensing)


## Installation <a name='installation'></a>

The code in the Jupyter Notebook for the analysis has been written in **Python 3**, and is using the packages **Numpy, Pandas, Matplotlib**, all of which are included in the Anaconda distribution of Python.

In addition, **os, json and glob** are used to load and write the outputs generated by the model.

The instance segmentation model is using Facebook's [Detectron2](https://github.com/facebookresearch/detectron2), which can be installed using pip directly from a forked repository.

[TensorBoard](https://www.tensorflow.org/tensorboard) was used for the visualization of the metrics recorded during the training of the model.

The processing of the segmentations (masks) is done using **OpenCV for Python**, which is also used to visualise annotations and predictions on the pictures. 

NOTE: the notebook was written in Google Colab to make use of its free access to GPU to train the model, therefore the function **cv2.imshow** from OpenCV is always replaced by its Colab equivalent **cv2_imshow**, as the former makes Colab crash.

The pictures for the project were annotated using the online tool [Make Sense](https://www.makesense.ai/).


## Project Motivation <a name='motivation'></a>

This project was done as Capstone Project for the Udacity Data Scientist Nanodegree. The aim is to use Detectron2's instance segmentation model to train it to retrieve the masks of coated peanuts to simulate a visual quality control.

In the food industry, the visual inspection of food chunks for quality control is still a manual process for many companies. Checking product characteristics like piece dimensions and proportion of defects to ensure an optimal consumer experience can be a very time-consuming task, and therefore expensive (in addition to being potentially unreliable). A lot of the visual checks performed by humans could actually be automated using state-of-the-art neural networks.

In this particular example, our objective is to:

* **Train an instance segmentation model from the Detectron2 model zoo to retrieve masks of coated peanuts on a picture.**
* **Evaluate that model to check its performance according to the [COCO metrics](https://cocodataset.org/#detection-eval).**
* **Use OpenCV on the output of the model to process the dimensions of the instances, and create visualizations of the quality metrics (proportion of defects / dimensions).**

The code to fulfill these objectives was created on a Python notebook in Google Colab, as a GPU was necessary for the training of the models. The .ipynb notebook file containing all the code can be found on this repository.


## File Descriptions <a name='files'></a>

* "Food_Chunk_Analyzer.ipynb": Jupyter notebook in which all the code to perform the tasks listed above can be found. Markdown cells throughout the code aim to guide the reader through the analysis and the choices that were made. NOTE: the notebook was written and used in Google Colab, with access to the training pictures and model outputs and weights via a mounted Google drive. If you wish to repeat the analysis using the notebook, make sure to change the paths to the training and testing datasets, as well as to the model files created.
* "peanut_snack.zip": zip file containing two folders, **train** and **val**. **train** contains all of the pictures that were used in the training set for the instance segmentation model, while **val** contains all the pictures that were used for the evaluation of the model. Each folder contains a .json file "labels" where the corresponding segmentation annotations are recorded in a VGG json format.
* "Model training outputs.zip": zip file containing one folder per trained model (i.e. per number of training iterations), with in each folder the metrics recorded during training, as well as the tensorboard events record.
* "Test results.zip": zip file containing one folder per trained model (i.e. per number of training iterations), with in each folder a .json file containing the COCO metrics from the evaluation of the model for a range of detection thresholds. 


## Results <a name='results'></a>

Results and analysis are concisely explained in the Jupyter Notebook.

A much more comprehensive explanation of the approach and method followed, as well as an analysis of the results, can be found in [this](link to come) Medium blogpost I wrote.


## Licensing, Authors, and Acknowledgements <a name='licensing'></a>

A big thanks to all the team working on Detectron2, which is released under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).

The pictures for the training of the model were taken by myself.
